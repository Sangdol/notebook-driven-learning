{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL 2. Common Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "|1000|\n",
      "|1001|\n",
      "|1002|\n",
      "|1003|\n",
      "|1004|\n",
      "|1005|\n",
      "|1006|\n",
      "|1007|\n",
      "|1008|\n",
      "|1009|\n",
      "+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "integerDF = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val integerDF = spark.range(1000, 10000)\n",
    "\n",
    "integerDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|     normalizedValue|\n",
      "+----+--------------------+\n",
      "|1000|                 0.0|\n",
      "|1001|1.111234581620180...|\n",
      "|1002|2.222469163240360...|\n",
      "|1003| 3.33370374486054E-4|\n",
      "|1004|4.444938326480720...|\n",
      "|1005|  5.5561729081009E-4|\n",
      "|1006| 6.66740748972108E-4|\n",
      "|1007| 7.77864207134126E-4|\n",
      "|1008|8.889876652961441E-4|\n",
      "|1009|0.001000111123458162|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colMin = 1000\n",
       "colMax = 9999\n",
       "normalizedIntegerDF = [id: bigint, normalizedValue: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, normalizedValue: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, max, min}\n",
    "\n",
    "val colMin = integerDF.select(min(\"id\")).first()(0).asInstanceOf[Long]\n",
    "val colMax = integerDF.select(max(\"id\")).first()(0).asInstanceOf[Long]\n",
    "\n",
    "val normalizedIntegerDF = integerDF\n",
    "  .withColumn(\"normalizedValue\", (col(\"id\") - colMin) / (colMax - colMin) )\n",
    "\n",
    "normalizedIntegerDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Null or Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+\n",
      "|hour|temperature|wind|\n",
      "+----+-----------+----+\n",
      "|  11|         66|   5|\n",
      "|  12|         68|null|\n",
      "|   1|       null|   6|\n",
      "|   2|         72|   7|\n",
      "+----+-----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "corruptDF = [hour: int, temperature: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[hour: int, temperature: int ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corruptDF = Seq(\n",
    "  (Some(11), Some(66), Some(5)),\n",
    "  (Some(12), Some(68), None),\n",
    "  (Some(1), None, Some(6)),\n",
    "  (Some(2), Some(72), Some(7))\n",
    ").toDF(\"hour\", \"temperature\", \"wind\")\n",
    "\n",
    "corruptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+\n",
      "|hour|temperature|wind|\n",
      "+----+-----------+----+\n",
      "|  11|         66|   5|\n",
      "|   2|         72|   7|\n",
      "+----+-----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "corruptDroppedDF = [hour: int, temperature: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[hour: int, temperature: int ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop any records that have null values.\n",
    "val corruptDroppedDF = corruptDF.na.drop(\"any\")\n",
    "\n",
    "corruptDroppedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+\n",
      "|hour|temperature|wind|\n",
      "+----+-----------+----+\n",
      "|  11|         66|   5|\n",
      "|  12|         68|   6|\n",
      "|   1|         68|   6|\n",
      "|   2|         72|   7|\n",
      "+----+-----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "map = Map(temperature -> 68, wind -> 6)\n",
       "corruptImputedDF = [hour: int, temperature: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[hour: int, temperature: int ... 1 more field]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Impute values with the mean\n",
    "val map = Map(\"temperature\" -> 68, \"wind\" -> 6)\n",
    "val corruptImputedDF = corruptDF.na.fill(map)\n",
    "\n",
    "corruptImputedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+\n",
      "|   id|   name|favorite_color|\n",
      "+-----+-------+--------------+\n",
      "|15342|  Conor|           red|\n",
      "|15342|  conor|           red|\n",
      "|12512|Dorothy|          blue|\n",
      "| 5234|   Doug|          aqua|\n",
      "+-----+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duplicateDF = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duplicateDF = Seq(\n",
    "  (15342, \"Conor\", \"red\"),\n",
    "  (15342, \"conor\", \"red\"),\n",
    "  (12512, \"Dorothy\", \"blue\"),\n",
    "  (5234, \"Doug\", \"aqua\")\n",
    "  ).toDF(\"id\", \"name\", \"favorite_color\")\n",
    "\n",
    "duplicateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+\n",
      "|   id|   name|favorite_color|\n",
      "+-----+-------+--------------+\n",
      "|15342|  Conor|           red|\n",
      "| 5234|   Doug|          aqua|\n",
      "|12512|Dorothy|          blue|\n",
      "+-----+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duplicateDedupedDF = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duplicateDedupedDF = duplicateDF.dropDuplicates(\"id\", \"favorite_color\")\n",
    "\n",
    "duplicateDedupedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Helpful Data Manipulation Functions\n",
    "\n",
    "| Function    | Use                                                                                                                        |\n",
    "|:------------|:---------------------------------------------------------------------------------------------------------------------------|\n",
    "| `explode()` | Returns a new row for each element in the given array or map                                                               |\n",
    "| `pivot()`   | Pivots a column of the current DataFrame and perform the specified aggregation                                             |\n",
    "| `cube()`    | Create a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them   |\n",
    "| `rollup()`  | Create a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
